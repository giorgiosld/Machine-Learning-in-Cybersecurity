import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import eagerpy as ep

from foolbox import TensorFlowModel, accuracy, samples, Model, TargetedMisclassification
from foolbox.attacks import L1FMNAttack, L2FMNAttack, LInfFMNAttack, L0FMNAttack


class MNISTAdversarialAttacker:
    def __init__(self, model_path):
        self.model = tf.keras.models.load_model(model_path)
        self.preprocessing = dict()
        self.fmodel: Model = TensorFlowModel(self.model, bounds=(0, 1), preprocessing=self.preprocessing)
        (self.mnist_images, self.mnist_labels), _ = tf.keras.datasets.mnist.load_data()
        self.mnist_images = self._preprocess_images(self.mnist_images)
        self.attacks = self._initialize_attacks()

    def _preprocess_images(self, images):
        """Preprocess MNIST images to be in the appropriate format."""
        images = images.astype(np.float32) / 255.0
        return np.expand_dims(images, axis=-1)

    def _initialize_attacks(self):
        """Initialize different types of attacks to be used."""
        attack_params = [0.01, 0.1, 1, 10, 100]
        return [
            (L1FMNAttack(), "L1FMNAttack", attack_params),
            (L2FMNAttack(), "L2FMNAttack", attack_params),
            (LInfFMNAttack(), "LInfFMNAttack", attack_params),
            (L0FMNAttack(), "L0FMNAttack", None)
        ]

    def run_attacks(self):
        """Run all configured attacks on MNIST samples."""
        for attack, attack_name, epsilons in self.attacks:
            self._perform_attack(attack, attack_name, epsilons)

    def _perform_attack(self, attack, attack_name, epsilons):
        """Perform a specific attack and visualize the results."""
        fig, axes = plt.subplots(10, 10, figsize=(10, 10))
        fig.suptitle(f'Adversarial Examples Grid - {attack_name}', fontsize=16)

        for original_label in range(10):
            images, labels = self._get_sample_for_label(original_label)
            self._display_original_image(axes, original_label, images)

            for target_label in range(10):
                if target_label == original_label:
                    continue

                print(f"\nOriginal label: {original_label}, Target label: {target_label}")
                self._attack_target_label(attack, images, labels, target_label, epsilons, axes, original_label)

        self._finalize_plot(fig, attack_name)

    def _get_sample_for_label(self, label):
        """Get a single sample image and label tensor for a given label."""
        index = np.where(self.mnist_labels == label)[0][0]
        images = ep.astensor(tf.convert_to_tensor(self.mnist_images[index:index + 1]))
        labels = ep.astensor(tf.convert_to_tensor(np.array([self.mnist_labels[index]], dtype=np.int32)))
        return images, labels

    def _display_original_image(self, axes, original_label, images):
        """Display the original image on the plot grid."""
        ax = axes[original_label, original_label]
        original_image = np.clip(images[0].numpy() * 255, 0, 255).astype(np.uint8).squeeze()
        ax.imshow(original_image, cmap='gray')
        ax.axis('off')
        ax.set_ylabel(f'{original_label}', fontsize=8, rotation=0, labelpad=20)

    def _attack_target_label(self, attack, images, labels, target_label, epsilons, axes, original_label):
        """Attempt an adversarial attack towards a target label."""
        criteria = TargetedMisclassification(
            ep.astensor(tf.convert_to_tensor(np.array([target_label], dtype=np.int32))))

        try:
            raw_adversarial, clipped_adversarial, success = attack(self.fmodel, images, criteria, epsilons=epsilons)
            self._evaluate_attack(axes, original_label, target_label, raw_adversarial, clipped_adversarial, success,
                                  epsilons, images, labels)
        except Exception as e:
            print(f"Error during attack: {str(e)}")

    def _evaluate_attack(self, axes, original_label, target_label, raw_adversarial, clipped_adversarial, success,
                         epsilons, images, labels):
        """Evaluate the success of an attack and display the adversarial image."""
        best_adversarial, attack_succeeded = self._get_best_adversarial(raw_adversarial, clipped_adversarial, success,
                                                                        epsilons, images, labels)

        if best_adversarial is not None:
            ax = axes[original_label, target_label]
            adversarial_display = np.clip(best_adversarial * 255, 0, 255).astype(np.uint8).squeeze()
            ax.imshow(adversarial_display, cmap='gray')
            ax.axis('off')
            ax.patch.set_edgecolor('red' if not attack_succeeded else 'green')
            ax.patch.set_linewidth(2)

    def _get_best_adversarial(self, raw_adversarial, clipped_adversarial, success, epsilons, images, labels):
        """Get the best adversarial example generated by the attack."""
        if epsilons is None:
            return self._check_attack_result(raw_adversarial, images, labels)

        best_adversarial = None
        attack_succeeded = False
        for eps, advs_, succ_ in zip(epsilons, clipped_adversarial, success):
            adversarial, success = self._check_attack_result(advs_, images, labels, eps)
            if adversarial is not None:
                best_adversarial = adversarial
                if success:
                    attack_succeeded = True
                    break
        return best_adversarial, attack_succeeded

    def _check_attack_result(self, advs_, images, labels, eps=None):
        """Check if the attack succeeded in creating an adversarial example."""
        if advs_ is None or (isinstance(advs_, ep.Tensor) and np.prod(advs_.shape) == 0):
            print("  Attack failed to produce adversarial examples")
            return None, False

        if not isinstance(advs_, ep.Tensor):
            advs_ = ep.astensor(tf.convert_to_tensor(advs_))

        if len(advs_.shape) != 4:
            print("  Invalid adversarial example shape")
            return None, False

        try:
            acc2 = accuracy(self.fmodel, advs_, labels)
            success = acc2 < 1.0

            if eps is None:
                modified_pixels = self._calculate_l0_norm(advs_, images)
                print(f"  L0 norm: modified pixels = {modified_pixels[0]}, accuracy = {acc2 * 100:4.1f}%")
            else:
                perturbation_sizes = (advs_ - images).norms.linf(axis=(1, 2, 3)).numpy()
                print(f"  norm â‰¤ {eps:<6}: {acc2 * 100:4.1f} %  perturbation:", ' '.join(map(str, perturbation_sizes)))

            predicted_class, confidence = self._get_prediction_details(advs_)
            print(f"  Predicted as class {predicted_class} with {confidence:.2f}% confidence")

            return advs_[0].numpy(), success
        except Exception as e:
            print(f"  Error processing attack result: {str(e)}")
            return None, False

    def _calculate_l0_norm(self, advs_, images):
        """Calculate the L0 norm of the perturbation."""
        diff = (advs_ - images).numpy()
        return np.sum(np.abs(diff) > 1e-10, axis=(1, 2, 3))

    def _get_prediction_details(self, advs_):
        """Get prediction class and confidence of the adversarial example."""
        pred = self.fmodel(advs_).numpy()
        predicted_class = np.argmax(pred[0])
        confidence = pred[0][predicted_class] * 100
        return predicted_class, confidence

    def _finalize_plot(self, fig, attack_name):
        """Finalize the plot layout and save the figure."""
        fig.text(0.5, 0.92, 'Output classification', ha='center', fontsize=12)
        fig.text(0.04, 0.5, 'Input class', va='center', rotation='vertical', fontsize=12)
        plt.tight_layout()
        plt.subplots_adjust(top=0.85, left=0.1, bottom=0.1)
        plt.savefig(f'resources/targeted_misclassification_{attack_name}.png')
        plt.show()


if __name__ == "__main__":
    attacker = MNISTAdversarialAttacker("mnist_cnn_model.h5")
    attacker.run_attacks()

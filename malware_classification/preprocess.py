import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np
import csv

class Preprocessor:
    def __init__(self, api_trace_file, label_file, ngram_range=(1, 1)):
        """
        Initializes the Preprocessor with the paths to the API trace file and label file
        :param api_trace_file: Path to the API traces
        :param label_file: Path to the labels
        """
        self.api_trace_file = api_trace_file
        self.label_file = label_file
        self.api_traces_df = None
        self.vectorizer = None
        self.ngram_range = ngram_range

    def load_data(self):
        """Loads the API traces and labels into a DataFrame"""
        # Load the API traces and labels into a DataFrame
        with open(self.api_trace_file, 'r') as f:
            api_traces = [line.strip() for line in f]
        with open(self.label_file, 'r') as f:
            labels = [line.strip() for line in f]

        self.api_traces_df = pd.DataFrame({
            'api_traces': api_traces,
            'labels': labels
        })

    @staticmethod
    def _remove_consecutive_duplicates(trace):
        """Helper function to remove consecutive duplicates from a trace"""
        calls = trace.split(',')
        result = [calls[0]]
        for i in range(1, len(calls)):
            if calls[i] != calls[i - 1]:
                result.append(calls[i])
        return ','.join(result)

    def remove_repeated_calls(self):
        """Remove consecutive repeated API calls within each trace (e.g., '1,1,2,3,3,1,1,4' becomes '1,2,3,1,4')"""
        self.api_traces_df['api_traces'] = self.api_traces_df['api_traces'].apply(self._remove_consecutive_duplicates)

    def vectorize_traces(self):
        """Convert traces into feature vectors using Bag of Words"""
        print("Using n-grams with ngram_range", self.ngram_range)
        self.vectorizer = CountVectorizer(ngram_range=self.ngram_range)
        X = self.vectorizer.fit_transform(self.api_traces_df['api_traces'])
        return X.toarray()

    def split_data(self, X, test_size=0.2, random_state=42):
        """Splits the data into training and test sets
        :param X: Feature vectors
        :param test_size: Fraction of data to reserve for testing
        :param random_state: Seed for random number generator
        """
        y = self.api_traces_df['labels'].tolist()
        return train_test_split(X, y, test_size=test_size, random_state=random_state)

    def visualize_class_distribution(self):
        """Visualize the class distribution of the labels"""
        labels, counts = np.unique(self.api_traces_df['labels'], return_counts=True)
        plt.bar(labels, counts)
        plt.xlabel('Classes')
        plt.ylabel('Count')
        plt.title('Class Distribution')
        plt.show()

    def save_traces_to_csv(self, output_file='api_trace_frequencies.csv'):
        """Save all traces and their frequencies to a CSV file to see the most common API calls and how much the frequency are
        unbalanced"""
        if not self.vectorizer:
            raise ValueError("Vectorizer must be initialized before calling this function.")

        # Transform the traces and calculate frequencies
        X = self.vectorizer.transform(self.api_traces_df['api_traces'])
        sum_words = np.sum(X, axis=0)
        words_freq = [(word, sum_words[0, idx]) for word, idx in self.vectorizer.vocabulary_.items()]
        words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)

        # Write the API call numbers and frequencies to CSV
        with open(output_file, mode='w', newline='') as file:
            writer = csv.writer(file)
            writer.writerow(['API Call', 'Frequency'])
            for word, freq in words_freq:
                writer.writerow([word, freq])

        print(f"All API traces and their frequencies saved to {output_file}.")

    def preprocess(self):
        """Main preprocessing pipeline"""
        print("Loading data...")
        self.load_data()
        print("Starting preprocessing...")
        # print("First 5 rows before removing repeated calls:\n", self.api_traces_df.head().to_string(index=False))
        self.remove_repeated_calls()
        # print("First 5 rows after removing repeated calls:\n", self.api_traces_df.head().to_string(index=False))
        return self.split_data(self.vectorize_traces())



if __name__ == '__main__':
    preprocessor = Preprocessor('api_trace.csv', 'apt_trace_labels.txt')
    X_train, X_test, y_train, y_test = preprocessor.preprocess()
    preprocessor.visualize_class_distribution()
    # preprocessor.save_traces_to_csv()
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer

import time
import csv

class Preprocessor:
    """
    A class to preprocess malware API traces for classification.

    Attributes:
    api_trace_file (str): Path to the file containing API traces
    label_file (str): Path to the file containing labels
    api_traces (list): List of API traces
    labels (list): List of labels
    vectorizer (CountVectorizer): CountVectorizer object for converting traces to feature vectors
    """

    def __init__(self, api_trace_file, label_file):
        """
        Initializes the Preprocessor with the paths to the API trace file and label file.
        :param api_trace_file:
        :param label_file:
        """
        self.api_trace_file = api_trace_file
        self.label_file = label_file
        self.api_traces = []
        self.labels = []
        self.vectorizer = None

    def load_data(self):
        """Loads the API traces and corresponding labels.
        """
        with open(self.api_trace_file, 'r') as f:
            for line in f:
                self.api_traces.append(line.strip())
        with open(self.label_file, 'r') as f:
            self.labels = [line.strip() for line in f]
        # with open(self.api_trace_file, 'r') as api_f, open(self.label_file, 'r') as label_f:
        #     self.api_traces = [line.strip() for line in api_f]
        #     self.labels = [line.strip() for line in label_f]

    @staticmethod
    def _remove_duplicates(trace):
        """Remove repeated API calls from a trace.
        :param trace: A string containing API calls separated by commas
        """
        api_calls = trace.split(',')
        return ','.join(api_calls[i] for i in range(len(api_calls)) if i == 0 or api_calls[i] != api_calls[i - 1])

    def preprocess_traces(self):
        """Preprocess the traces by removing duplicate calls."""
        for i, trace in enumerate(self.api_traces):
            self.api_traces[i] = self._remove_duplicates(trace)

    def vectorize_traces(self):
        """Convert traces into feature vectors using Bag of Words."""
        # Use CountVectorizer with binary=True to implement bag-of-words and ignore frequencies
        self.vectorizer = CountVectorizer(tokenizer=lambda x: x.split(','), binary=True)
        X = self.vectorizer.fit_transform(self.api_traces)
        return X.toarray()

    def split_data(self, X, y, test_size=0.2, random_state=42):
        """Splits the data into training and test sets.
        :param X: Feature vectors
        :param y: Labels
        :param test_size: Fraction of data to reserve for testing
        :param random_state: Seed for random number generator
        """
        return train_test_split(X, y, test_size=test_size)

    def visualize_class_distribution(self):
        """Visualize the class distribution of the labels."""
        labels, counts = np.unique(self.labels, return_counts=True)
        plt.bar(labels, counts)
        plt.xlabel('Classes')
        plt.ylabel('Count')
        plt.title('Class Distribution')
        plt.show()

    def save_traces_to_csv(self, output_file='api_trace_frequencies.csv'):
        """Save all API traces and their frequencies to a CSV file."""
        if not self.vectorizer:
            raise ValueError("Vectorizer must be initialized before calling this function.")

        # Transform the traces and calculate frequencies
        X = self.vectorizer.transform(self.api_traces)
        sum_words = np.sum(X, axis=0)
        words_freq = [(word, sum_words[0, idx]) for word, idx in self.vectorizer.vocabulary_.items()]
        words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)  # Sort by frequency

        # Write the API call numbers and frequencies to CSV
        with open(output_file, mode='w', newline='') as file:
            writer = csv.writer(file)
            writer.writerow(['API Call', 'Frequency'])
            for word, freq in words_freq:
                writer.writerow([word, freq])

        print(f"All API traces and their frequencies saved to {output_file}.")

    def print_top_10_traces(self):
        """Print the top 10 most frequent API traces."""
        if not self.vectorizer:
            raise ValueError("Vectorizer must be initialized before calling this function.")

        # Transform the traces and calculate frequencies
        X = self.vectorizer.transform(self.api_traces)
        sum_words = np.sum(X, axis=0)
        words_freq = [(word, sum_words[0, idx]) for word, idx in self.vectorizer.vocabulary_.items()]
        words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)[:10]  # Top 10 API calls

        # Print the API call values and frequencies
        print("Top 10 Most Frequent API Calls with Their Frequencies:")
        for word, freq in words_freq:
            print(f"API Call: {word}, Frequency: {freq}")

        # Now plot the API calls
        words, freqs = zip(*words_freq)
        plt.bar(words, freqs)
        plt.xlabel('API Calls')
        plt.ylabel('Frequency')
        plt.title('Top 10 Most Common API Calls')
        plt.xticks(rotation=90)
        plt.show()

    def preprocess(self):
        """Main preprocessing pipeline"""
        self.load_data()
        X = self.vectorize_traces()
        return self.split_data(X, self.labels)

if __name__ == '__main__':
    t = time.time()
    preprocessor = Preprocessor('api_trace.csv', 'apt_trace_labels.txt')
    X_train, X_test, y_train, y_test = preprocessor.preprocess()
    preprocessor.visualize_class_distribution()
    preprocessor.save_traces_to_csv()
    preprocessor.print_top_10_traces()
    print(f'Total samples: {len(preprocessor.api_traces)}')
    print(f'Training samples: {len(X_train)}')
    print(f'Test samples: {len(X_test)}')
    print(f'Elapsed time: {time.time() - t:.2f} seconds')
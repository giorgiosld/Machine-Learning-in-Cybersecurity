import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np
import csv

class Preprocessor:
    def __init__(self, api_trace_file, label_file):
        """
        Initializes the Preprocessor with the paths to the API trace file and label file
        :param api_trace_file: Path to the API traces
        :param label_file: Path to the labels
        """
        self.api_trace_file = api_trace_file
        self.label_file = label_file
        self.api_traces_df = None
        self.vectorizer = None

    def load_data(self):
        """Loads the API traces and labels into a DataFrame"""
        # Load the API traces and labels into a DataFrame
        with open(self.api_trace_file, 'r') as f:
            api_traces = [line.strip() for line in f]
        with open(self.label_file, 'r') as f:
            labels = [line.strip() for line in f]

        self.api_traces_df = pd.DataFrame({
            'api_traces': api_traces,
            'labels': labels
        })

    def drop_duplicates(self):
        """Remove duplicate rows based on the 'api_traces' column using Pandas"""
        self.api_traces_df.drop_duplicates(subset=['api_traces'], inplace=True)

    def vectorize_traces(self):
        """Convert traces into feature vectors using Bag of Words"""
        self.vectorizer = CountVectorizer()
        X = self.vectorizer.fit_transform(self.api_traces_df['api_traces'])
        return X.toarray()

    def split_data(self, X, test_size=0.2, random_state=42):
        """Splits the data into training and test sets
        :param X: Feature vectors
        :param test_size: Fraction of data to reserve for testing
        :param random_state: Seed for random number generator
        """
        y = self.api_traces_df['labels'].tolist()  # Extract labels
        return train_test_split(X, y, test_size=test_size, random_state=random_state)

    def visualize_class_distribution(self):
        """Visualize the class distribution of the labels"""
        labels, counts = np.unique(self.api_traces_df['labels'], return_counts=True)
        plt.bar(labels, counts)
        plt.xlabel('Classes')
        plt.ylabel('Count')
        plt.title('Class Distribution')
        plt.show()

    def save_traces_to_csv(self, output_file='api_trace_frequencies.csv'):
        """Save all traces and their frequencies to a CSV file to see the most common API calls and how much the frequency are
        unbalanced"""
        if not self.vectorizer:
            raise ValueError("Vectorizer must be initialized before calling this function.")

        # Transform the traces and calculate frequencies
        X = self.vectorizer.transform(self.api_traces_df['api_traces'])
        sum_words = np.sum(X, axis=0)
        words_freq = [(word, sum_words[0, idx]) for word, idx in self.vectorizer.vocabulary_.items()]
        words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)  # Sort by frequency

        # Write the API call numbers and frequencies to CSV
        with open(output_file, mode='w', newline='') as file:
            writer = csv.writer(file)
            writer.writerow(['API Call', 'Frequency'])
            for word, freq in words_freq:
                writer.writerow([word, freq])

        print(f"All API traces and their frequencies saved to {output_file}.")

    def preprocess(self):
        """Main preprocessing pipeline"""
        self.load_data()
        self.drop_duplicates()
        X = self.vectorize_traces()
        return self.split_data(X)


if __name__ == '__main__':
    preprocessor = Preprocessor('api_trace.csv', 'apt_trace_labels.txt')
    X_train, X_test, y_train, y_test = preprocessor.preprocess()
    preprocessor.visualize_class_distribution()
    preprocessor.save_traces_to_csv()
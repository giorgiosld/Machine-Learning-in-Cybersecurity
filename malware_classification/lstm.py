from sklearn.metrics import f1_score
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.optimizers import Adam

from preprocess import Preprocessor

class LSTMClassifier:
    def __init__(self, vocab_size, embedding_dim=100, lstm_units=256, dropout_rate=0.3, learning_rate=0.0005):
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.lstm_units = lstm_units
        self.dropout_rate = dropout_rate
        self.learning_rate = learning_rate
        self.model = self._build_model()
        self.label_encoder = LabelEncoder()

    def _build_model(self):
        model = Sequential()
        model.add(Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim))
        model.add(LSTM(units=self.lstm_units, dropout=self.dropout_rate))
        model.add(Dense(1, activation='softmax'))
        model.compile(optimizer=Adam(learning_rate=self.learning_rate), loss='binary_crossentropy', metrics=['accuracy'])
        return model

    def fit(self, X_train, y_train, X_test, y_test, epochs=10, batch_size=32):
        """Trains the LSTM model."""
        # Ensure labels are integer-encoded using LabelEncoder
        y_train = self.label_encoder.fit_transform(y_train)
        y_test = self.label_encoder.transform(y_test)
        # Train the model
        self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))

    def evaluate(self, X_test, y_test):
        """Evaluates the LSTM model and returns F1 score."""
        # Predict on the test set
        y_test = self.label_encoder.transform(y_test)

        y_pred = self.model.predict(X_test)
        y_pred = y_pred.argmax(axis=1)  # Convert probabilities to class labels

        # Evaluate the F1 score using scikit-learn
        f1 = f1_score(y_test, y_pred, average='macro')
        print(f"LSTM Model F1 Score: {f1:.4f}")
        return f1

def create_lstm_model(vocab_size, embedding_dim, input_length, num_classes):
    """Creates an LSTM model."""
    model = Sequential()
    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length))
    model.add(LSTM(units=512, return_sequences=False))  # Adjust units as needed
    model.add(Dense(units=num_classes, activation='softmax'))
    model.compile(optimizer=Adam(learning_rate=0.0005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

def train_lstm_model(api_trace_file, label_file):
    """Trains the LSTM model on the given API traces and labels."""
    preprocessor = Preprocessor(api_trace_file, label_file)
    X_train, X_test, y_train, y_test = preprocessor.preprocess()

    # Ensure that labels are integers
    le = LabelEncoder()
    y_train = le.fit_transform(y_train)
    y_test = le.transform(y_test)

    input_dim = X_train.shape[1]  # Number of features (from CountVectorizer)
    embedding_dim = 100  # Adjust the embedding dimension if needed
    input_length = X_train.shape[1]  # Input length is the number of features
    num_classes = len(set(y_train))  # Number of unique classes

    # Create the LSTM model
    model = create_lstm_model(input_dim, embedding_dim, input_length, num_classes)

    # Train the model
    model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test))

    # Predict on the test set
    y_pred = model.predict(X_test)
    y_pred = y_pred.argmax(axis=1)

    f1 = f1_score(y_test, y_pred, average='macro')
    print(f"LSTM Model F1 Score: {f1:.4f}")


if __name__ == '__main__':
    train_lstm_model('api_trace.csv', 'apt_trace_labels.txt')
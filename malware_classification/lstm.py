from sklearn.metrics import f1_score
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam

from preprocess import Preprocessor

class LSTMClassifier:
    # def __init__(self, vocab_size, embedding_dim=100, lstm_units=256, dropout_rate=0.3, learning_rate=0.0005):
    def __init__(self, vocab_size, embedding_dim=100, lstm_units=512, dropout_rate=0.3, learning_rate=0.0005, num_classes=10):
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.lstm_units = lstm_units
        self.dropout_rate = dropout_rate
        self.learning_rate = learning_rate
        self.num_classes = num_classes
        self.model = self._build_model()
        self.label_encoder = LabelEncoder()

    def _build_model(self):
        # model = Sequential()
        # model.add(Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim))
        # model.add(LSTM(units=self.lstm_units, dropout=self.dropout_rate))
        # model.add(Dense(1, activation='softmax'))
        # model.compile(optimizer=Adam(learning_rate=self.learning_rate), loss='binary_crossentropy', metrics=['accuracy'])
        # return model
        model = Sequential()
        model.add(Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim))
        model.add(LSTM(units=self.lstm_units, return_sequences=True))
        model.add(Dropout(self.dropout_rate))  # Adding dropout
        model.add(LSTM(units=self.lstm_units))
        model.add(BatchNormalization())  # Batch normalization
        model.add(Dense(self.num_classes, activation='softmax'))  # Adjusted for multi-class classification
        model.compile(optimizer=Adam(learning_rate=self.learning_rate), loss='sparse_categorical_crossentropy',
                      metrics=['accuracy'])
        return model

    def fit(self, X_train, y_train, X_test, y_test, epochs=10, batch_size=32):
        """Trains the LSTM model."""
        # Ensure labels are integer-encoded using LabelEncoder
        y_train = self.label_encoder.fit_transform(y_train)
        y_test = self.label_encoder.transform(y_test)
        # Train the model
        self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))

    def evaluate(self, X_test, y_test):
        """Evaluates the LSTM model and returns F1 score."""
        # Predict on the test set
        y_test = self.label_encoder.transform(y_test)

        y_pred = self.model.predict(X_test)
        y_pred = y_pred.argmax(axis=1)

        # Evaluate the F1 score using scikit-learn
        f1 = f1_score(y_test, y_pred, average='macro')
        print(f"LSTM Model F1 Score: {f1:.4f}")
        return f1

